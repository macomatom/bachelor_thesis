{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import concurrent.futures\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "from threading import Event\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "LIMIT = 20\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}\n",
    "FOLDER = \"C:/Matko - CloudStation/UPJS/_Bakalarka/data_drojove_kody_stranok_nove/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50769, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                    name  \\\nindex                                                      \n101248                      Erasmus University Rotterdam   \n101249                                        LifeStream   \n101252                           Shirak State University   \n101255                       Radboud University Nijmegen   \n101257  Northamptonshire Healthcare NHS Foundation Trust   \n\n                              link        type  lang_eng  \nindex                                                     \n101248  http://www.eur.nl/english/   Education         1  \n101249    https://www.lstream.org/   Nonprofit         1  \n101252          http://shsu.am/en/   Education         1  \n101255   http://www.ru.nl/english/   Education         1  \n101257    https://www.nhft.nhs.uk/  Healthcare         1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>link</th>\n      <th>type</th>\n      <th>lang_eng</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>101248</th>\n      <td>Erasmus University Rotterdam</td>\n      <td>http://www.eur.nl/english/</td>\n      <td>Education</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>101249</th>\n      <td>LifeStream</td>\n      <td>https://www.lstream.org/</td>\n      <td>Nonprofit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>101252</th>\n      <td>Shirak State University</td>\n      <td>http://shsu.am/en/</td>\n      <td>Education</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>101255</th>\n      <td>Radboud University Nijmegen</td>\n      <td>http://www.ru.nl/english/</td>\n      <td>Education</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>101257</th>\n      <td>Northamptonshire Healthcare NHS Foundation Trust</td>\n      <td>https://www.nhft.nhs.uk/</td>\n      <td>Healthcare</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_unreachable_urls_filtered_with_lang_labels.csv', )\n",
    "df = df.set_index('index')\n",
    "df = df[df['lang_eng'] == 1] # Pracujeme iba s anglickym jazykom\n",
    "print(df.shape)\n",
    "df.head()\n",
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# url_adresy = [[i, link] for i, link in enumerate(df['link']) if df.iloc[i, 4] == 1]\n",
    "# url_adresy = [[i, link] for i, link in zip(df[\"index\"], df[\"link\"]) if df.loc[df[\"index\"] == i][\"lang_eng\"].values[0] == 1]\n",
    "# len(url_adresy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_data_from_subpage(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=3, verify=False)\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def get_links_of_subpages(website_link):\n",
    "    newly_found_links = deque()\n",
    "    html_data = get_data_from_subpage(website_link)\n",
    "    if html_data is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True)[:LIMIT]:\n",
    "            # print(link)\n",
    "\n",
    "            # Append to list if new link contains original link\n",
    "            if str(link[\"href\"]).startswith((str(website_link))):\n",
    "                newly_found_links.append(link[\"href\"])\n",
    "\n",
    "            # Include all href that do not start with website link but with \"/\"\n",
    "            if str(link[\"href\"]).startswith(\"/\"):\n",
    "                link_with_www = website_link + link[\"href\"][1:]\n",
    "                newly_found_links.append(link_with_www)\n",
    "\n",
    "            # include subdomains i.e domain: upjs.sk, subdomain: science.upjs.sk\n",
    "            if website_link.split(\"//\")[1].split(\"/\")[0].replace(\"www.\", \"\") in str(link[\"href\"]):\n",
    "                newly_found_links.append(str(link[\"href\"]))\n",
    "        return newly_found_links, html_data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def get_subpages_of_website(website_url, i):\n",
    "    written_pages_iter = 0\n",
    "    links_to_be_checked = deque([website_url])\n",
    "    already_checked_links = deque()\n",
    "    # file_name = f\"../tmp//{i}.txt\"\n",
    "\n",
    "    while links_to_be_checked:\n",
    "        link = links_to_be_checked.popleft()\n",
    "        if link not in already_checked_links:\n",
    "            new_links_of_subpages, page_content = get_links_of_subpages(link)\n",
    "            if not (new_links_of_subpages is None and page_content is None):\n",
    "                links_to_be_checked.extend(new_links_of_subpages)\n",
    "                with open(f\"{FOLDER}{i}/{written_pages_iter}.txt\", \"w\", encoding='utf-8') as file:\n",
    "                    file.write(link+\"\\n\")\n",
    "                    file.write(str(page_content))\n",
    "                    written_pages_iter += 1\n",
    "\n",
    "                # Print some statements\n",
    "                total = len(links_to_be_checked) + len(already_checked_links)\n",
    "                # print(f\"Page_idx: {i}\")\n",
    "                # print(f\"Progress: {written_pages_iter} / {LIMIT}\\n\")\n",
    "                # print(f\"Checked: {len(already_checked_links)} / {total}\\n\")\n",
    "                # print(f\"Written: = {written_pages_iter}\")\n",
    "\n",
    "        already_checked_links.append(link)\n",
    "\n",
    "        # print(len(links_to_be_checked))\n",
    "        # print(links_to_be_checked)\n",
    "\n",
    "        if written_pages_iter >= LIMIT:\n",
    "            break\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def crawl(i, adresa):\n",
    "\n",
    "    folder_name = FOLDER + str(i)\n",
    "\n",
    "    try:\n",
    "        page = requests.get(adresa, headers=HEADERS, timeout=5)\n",
    "        if page.status_code == 200:\n",
    "\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.mkdir(folder_name)\n",
    "\n",
    "            get_subpages_of_website(adresa, i)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.info(f'{i}: {adresa}: is Not reachable')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 name                      link       type  \\\nindex                                                                        \n0      Australian National University    http://www.anu.edu.au/  Education   \n1                   Monash University    http://www.monash.edu/  Education   \n2            University of Queensland     http://www.uq.edu.au/  Education   \n3                Macquarie University         http://mq.edu.au/  Education   \n4                         UNSW Sydney  https://www.unsw.edu.au/  Education   \n\n       lang_eng  ma_20_suborov  \nindex                           \n0             1              1  \n1             1              1  \n2             1              1  \n3             1              1  \n4             1              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>link</th>\n      <th>type</th>\n      <th>lang_eng</th>\n      <th>ma_20_suborov</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Australian National University</td>\n      <td>http://www.anu.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Monash University</td>\n      <td>http://www.monash.edu/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>University of Queensland</td>\n      <td>http://www.uq.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Macquarie University</td>\n      <td>http://mq.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UNSW Sydney</td>\n      <td>https://www.unsw.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zistenie kolko podstranok mam stiahnutych ku ktorej url adrese\n",
    "\n",
    "df = df.reindex(columns=df.columns.tolist() + ['ma_20_suborov'])\n",
    "\n",
    "# Fill all values in column with 0 as default\n",
    "df['ma_20_suborov'] = 0\n",
    "\n",
    "for filename in os.listdir(FOLDER):\n",
    "    pocet_suborov = len(os.listdir(FOLDER + str(filename)))\n",
    "    if pocet_suborov == 20:\n",
    "        df.at[int(filename), 'ma_20_suborov'] = 1   # If there are 20 files in directory\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21469, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            name  \\\nindex                                              \n8                         University of Tasmania   \n24                        University of Canberra   \n31     QIMR Berghofer Medical Research Institute   \n34       Victor Chang Cardiac Research Institute   \n36                                Mater Research   \n\n                                   link       type  lang_eng  ma_20_suborov  \\\nindex                                                                         \n8               http://www.utas.edu.au/  Education         1              0   \n24          http://www.canberra.edu.au/  Education         1              0   \n31     http://www.qimrberghofer.edu.au/   Facility         1              0   \n34       http://www.victorchang.edu.au/  Nonprofit         1              0   \n36        http://research.mater.org.au/   Facility         1              0   \n\n       checked  \nindex           \n8            0  \n24           0  \n31           0  \n34           0  \n36           0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>link</th>\n      <th>type</th>\n      <th>lang_eng</th>\n      <th>ma_20_suborov</th>\n      <th>checked</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>University of Tasmania</td>\n      <td>http://www.utas.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>University of Canberra</td>\n      <td>http://www.canberra.edu.au/</td>\n      <td>Education</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>QIMR Berghofer Medical Research Institute</td>\n      <td>http://www.qimrberghofer.edu.au/</td>\n      <td>Facility</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Victor Chang Cardiac Research Institute</td>\n      <td>http://www.victorchang.edu.au/</td>\n      <td>Nonprofit</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Mater Research</td>\n      <td>http://research.mater.org.au/</td>\n      <td>Facility</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stranky (priecinky), ktorym prislucha menej ako 20 suborov\n",
    "\n",
    "df_recheck = df[df['ma_20_suborov'] == 0]\n",
    "\n",
    "# Novy stlpec na trackovanie, ci je url adresa skontrolovana\n",
    "df_recheck = df_recheck.reindex(columns=df_recheck.columns.tolist() + ['checked'])\n",
    "df_recheck['checked'] = 0\n",
    "\n",
    "print(df_recheck.shape)\n",
    "df_recheck.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21469, 6)\n",
      "(21468, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_recheck.shape)\n",
    "df_recheck = df_recheck.drop(labels=16236, axis=0)\n",
    "print(df_recheck.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]WARNING:urllib3.connection:Certificate did not match expected hostname: www.uofk.edu. Certificate: {'subject': ((('commonName', 'uofk.edu'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '041EA742E2B013D1590C8B5B2C49C9AFDBCA', 'notBefore': 'Feb 15 06:15:13 2023 GMT', 'notAfter': 'May 16 06:15:12 2023 GMT', 'subjectAltName': (('DNS', 'uofk.edu'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n",
      "WARNING:urllib3.connection:Certificate did not match expected hostname: oceanleadership.org. Certificate: {'subject': ((('countryName', 'US'),), (('stateOrProvinceName', 'Colorado'),), (('organizationName', 'The University Corporation for Atmospheric Research'),), (('commonName', 'redirect.lweb.ucar.edu'),)), 'issuer': ((('countryName', 'US'),), (('stateOrProvinceName', 'MI'),), (('localityName', 'Ann Arbor'),), (('organizationName', 'Internet2'),), (('organizationalUnitName', 'InCommon'),), (('commonName', 'InCommon ECC Server CA'),)), 'version': 3, 'serialNumber': '6B9951FAB5895E747D33E5B4460E5DAE', 'notBefore': 'Nov 21 00:00:00 2022 GMT', 'notAfter': 'Nov 21 23:59:59 2023 GMT', 'subjectAltName': (('DNS', 'redirect.lweb.ucar.edu'), ('DNS', 'amp.ucar.edu'), ('DNS', 'apps.weg.ucar.edu'), ('DNS', 'asp.ucar.edu'), ('DNS', 'ccmi.ucar.edu'), ('DNS', 'chsp.ucar.edu'), ('DNS', 'dailyb.cisl.ucar.edu'), ('DNS', 'dash.ucar.edu'), ('DNS', 'dls.ucar.edu'), ('DNS', 'drupal.ucar.edu'), ('DNS', 'gtp.ucar.edu'), ('DNS', 'help.ucar.edu'), ('DNS', 'hiaper.ucar.edu'), ('DNS', 'hippo.ucar.edu'), ('DNS', 'isp.ucar.edu'), ('DNS', 'nwsc.ucar.edu'), ('DNS', 'president.ucar.edu'), ('DNS', 'raccoon.ucar.edu'), ('DNS', 'rchelp.ucar.edu'), ('DNS', 'rda.ucar.edu'), ('DNS', 'rdahelp.ucar.edu'), ('DNS', 'rmcwic.ucar.edu'), ('DNS', 'spark.ucar.edu'), ('DNS', 'submit-data.ucar.edu'), ('DNS', 'support.ucar.edu'), ('DNS', 'ucar.edu'), ('DNS', 'ucarconnect.ucar.edu'), ('DNS', 'ucp.ucar.edu'), ('DNS', 'vets.ucar.edu'), ('DNS', 'www.asp.ucar.edu'), ('DNS', 'www.dls.ucar.edu'), ('DNS', 'www.homme.ucar.edu'), ('DNS', 'www.isp.ucar.edu'), ('DNS', 'www.nesl.ucar.edu'), ('DNS', 'www.nwsc.ucar.edu'), ('DNS', 'www.spark.ucar.edu'), ('DNS', 'www.ucp.ucar.edu'), ('DNS', 'www.vets.ucar.edu'), ('DNS', 'www2.fin.ucar.edu'), ('DNS', 'www2.image.ucar.edu'), ('DNS', 'www2.ucar.edu'), ('DNS', 'yotc.ucar.edu')), 'OCSP': ('http://ocsp.incommon-ecc.org',), 'caIssuers': ('http://cert.incommon-ecc.org/InCommonECCServerCA.crt',), 'crlDistributionPoints': ('http://crl.incommon-ecc.org/InCommonECCServerCA.crl',)}\n",
      "WARNING:urllib3.connectionpool:Failed to parse headers (url=https://apply.montevallo.edu:443/Apply/): [MissingHeaderBodySeparatorDefect()], unparsed data: 'X-Frame Options: ALLOW FROM https://crmapp-prod.montevallo.edu\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Permitted-Cross-Domain-Policies: none\\r\\nDate: Wed, 01 Mar 2023 07:16:07 GMT\\r\\nContent-Length: 159\\r\\n\\r\\n'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Martin\\DataspellProjects\\bakalarka\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 469, in _make_request\n",
      "    assert_header_parsing(httplib_response.msg)\n",
      "  File \"C:\\Users\\Martin\\DataspellProjects\\bakalarka\\venv\\Lib\\site-packages\\urllib3\\util\\response.py\", line 91, in assert_header_parsing\n",
      "    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)\n",
      "urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'X-Frame Options: ALLOW FROM https://crmapp-prod.montevallo.edu\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Permitted-Cross-Domain-Policies: none\\r\\nDate: Wed, 01 Mar 2023 07:16:07 GMT\\r\\nContent-Length: 159\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# Samotne stahovanie udajov zo stranok\n",
    "\n",
    "od_idx, po_idx = 0, 10000\n",
    "n_jobs = po_idx - od_idx\n",
    "\n",
    "indexes = np.array(list(df_recheck.index.values))[od_idx:po_idx]\n",
    "urls = np.array(list(df_recheck['link']))[od_idx:po_idx]\n",
    "\n",
    "start = time.perf_counter() # calculate the time\n",
    "\n",
    "with tqdm(total=(n_jobs)) as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=500) as executor: # In this case thread would be better\n",
    "        # futures = {executor.submit(crawl, i, url): (i, url) for i, url in np.array(df)[od_idx:po_idx]} # cely dataset\n",
    "        futures = {executor.submit(crawl, i, url): (i, url) for i, url in zip(indexes, urls)} # len stranky, ktore opakovane stahujeme\n",
    "        results = {}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            arg = futures[future]\n",
    "            results[arg] = future.result()\n",
    "            pbar.update(1)\n",
    "\n",
    "end = time.perf_counter() # calculate finish time\n",
    "print(f'Threads: Finished in {round(end - start,2)} second(s)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Nepouzity kod multiprocesingu\n",
    "\n",
    "# # Vlozenie jednotlivych taskov (stranok) do radu (Queue)\n",
    "# threads = []\n",
    "# q = Queue(maxsize=0)\n",
    "# # n_jobs = min(1000000000, len(url_adresy))\n",
    "# from_n_jobs, n_jobs = 0, 10000\n",
    "#\n",
    "#\n",
    "# # results = [{} for x in url_adresy]\n",
    "# results = [False for x in url_adresy]\n",
    "# for i, url in url_adresy[from_n_jobs: n_jobs]:\n",
    "#     q.put((i, url))\n",
    "#\n",
    "#\n",
    "# # Vykonanie taskov pomocou multithreading-u\n",
    "# cas = time.time()\n",
    "#\n",
    "# #Starting worker threads on queue processing\n",
    "# for i in range(n_jobs):\n",
    "#     logging.debug('Starting thread ', i)\n",
    "#     worker = Thread(target=crawl, args=(q, results))\n",
    "#     # worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to exit eventually even if these dont finish correctly.\n",
    "#     worker.daemon = True\n",
    "#     worker.start()\n",
    "#\n",
    "#\n",
    "# #now we wait until the queue has been processed\n",
    "# q.join()\n",
    "#\n",
    "# # logging.info('All tasks completed.')\n",
    "# print('All tasks completed.')\n",
    "#\n",
    "# print(f\"Websites checked: {n_jobs}\")\n",
    "# print(f\"Finished in {(time.time() - cas):.1f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Odstranenie vsetkych suborov z lubovolneho priecinku definovaneho ako \"folder\"\n",
    "\n",
    "folder = '../tmp'  # Change me\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder count: 46430\n",
      "{20: 29322, 1: 5989, 17: 1178, 19: 1161, 0: 152, 18: 1238, 2: 785, 5: 169, 9: 276, 16: 1055, 13: 650, 8: 247, 3: 354, 4: 223, 15: 976, 6: 203, 11: 501, 10: 405, 14: 739, 7: 253, 12: 554}\n",
      "29322\n"
     ]
    }
   ],
   "source": [
    "# Statistika - kolko suborov je v jednotlivych priecinkoch\n",
    "\n",
    "d = {}\n",
    "\n",
    "websites_with_20_subpages_downloaded = []\n",
    "\n",
    "print(f\"Folder count: {len(os.listdir(FOLDER))}\")\n",
    "for filename in os.listdir(FOLDER):\n",
    "    pocet_suborov = len(os.listdir(f'{FOLDER}{filename}'))\n",
    "    if pocet_suborov == 20:\n",
    "        websites_with_20_subpages_downloaded.append(int(filename))\n",
    "    # print(f\"{filename}: {pocet_suborov}\")\n",
    "    d[pocet_suborov] = d[pocet_suborov] + 1 if pocet_suborov in d else 1\n",
    "\n",
    "print(d)\n",
    "print(len(websites_with_20_subpages_downloaded))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
